{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy String Matching at Scale\n",
    "\n",
    "### Entity Resolution is Everywhere, and Sometimes All We Got Are Strings\n",
    "![StringUrl](https://media.giphy.com/media/l3JDHiU3rdY4oBK3S/giphy.gif \"string\")  \n",
    "More and more often, companies are blending data from different sources to enhance and enrich the value we are getting from the data. Central to this effort is the concept of entity resolution (or record linkage) to ensure that we are looking at the same record across multiple different sources. In some cases the records may have enough different types of information that can be used to build a probabilistic estimate on whether it is the same entity. In other cases, we may only be looking at one field, such as a name, and we need to decide whether it is enough of a match or not.\n",
    "\n",
    "#### More Data = Need for Speed\n",
    "Fuzzy string matching is not a new problem, and several algorithms are commonly employed (Levenshtein distance, Jaroâ€“Winkler distance). However, given the growth in the number of data that are being matched, it is increasingly important to be able to perform this matching at scale. Instead of comparing every record to every possible match, we can employ a vectorized approach using Natural Language Processing (NLP) libraries that make this not only possible, but relatively painless in implementation.\n",
    "\n",
    "#### TF-IDF Approach\n",
    "I got excited about using the NLP toolkit for short string matching after reading about its success on [Chris van den Berg's Blog Post](https://bergvca.github.io/2017/10/14/super-fast-string-matching.html). TF-IDF stands for \"term frequency-inverse document frequency\" and is a common approach to measuring similarity/dissimilarity among documents in a corpus (a collection of documents). The TF-IDF calculations typically consist of the following steps:\n",
    "1. *Pre-processing & Tokenization*: Perform any cleaning on the data (case conversion, removal of stopwords & punctuation) and convert each document into tokens. Although tokenization is typically performed at the word level, we have the flexibility to define a token at a lower level, such as an n-gram, which is more useful for short string matching since we might only have a few words in each string.\n",
    "2. *Calculate the Term Frequency*: The purpose of this step is to determine which words define the document; words that appear more frequently are indicative of what the document's subject matter. For each document (a string in our case), calculate the frequency for each term (token) in the document and divide by the total number of terms in the document. If we define a token as an n-gram, we will calculate the frequency of each n-gram in our string.\n",
    "  \n",
    "$$TF(t) = (Number\\:of\\:times\\:term\\:t\\:appears\\:in\\:a\\:document) / (Total\\:number\\:of\\:terms\\:in\\:the\\:document)$$  \n",
    "\n",
    "3. *Calculate the Inverse Document Frequency*: The purpose of this step is to calculate the appropriate *weight* for each term, depending on how often it appears across all documents. A term that appears in all the different documents will have a lower weight compared to a term that only appears in one of the documents. The idea is that a token that appears in all documents is less is less descriptive of any particular document compared to a token that appears in only one of the documents.  \n",
    "\n",
    "$$IDF(t) = ln(Total number of documents / Number of documents with term t in it)$$\n",
    "\n",
    "4. *Calculate the TF-IDF Weights for each token*: Multiply the term frequency with the inverse document frequency\n",
    "5. *Calculate the Cosine Similarity*: Cosine similarity is often used to compare the similarity of two vectors (in this case TF-IDF values). As described by Chris van den Berg, data scientists at ING developed a custom library to make the cosine similarity calcualtions faster than the built-in sci-kit learn implementation, which you can read about more [here](https://medium.com/wbaa/https-medium-com-ingwbaa-boosting-selection-of-the-most-similar-entities-in-large-scale-datasets-450b3242e618). We will use this library for a faster cosine similarity calculation than the built-in scikit learn cosine_similarity function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Matching Film Titles\n",
    "Matching titles is a perfect use case, since in many cases there may not be much more than a name to use for matching and we need to find the best match against a medium-large data set. For this example, I will demonstrate the TF-IDF string matching approach by matching titles from the [MovieLens Kaggle dataset](https://www.kaggle.com/grouplens/movielens-latest-full#links.csv) to the [IMDB title dataset](https://www.imdb.com/interfaces/). The titles in the MovieLens dataset are very similar to the IMDB titles, with only a couple minor differences, as shown in the example below. The MovieLens Kaggle dataset is also ideal because it contains the IMDB Id key, which we can use to get the true match for every record to validate our best guess.\n",
    "\n",
    "MovieLens Title: \"Confessional, The (Confessionnal, Le) (1995)\"  \n",
    "IMDB Title: \"The Confessional (1995)\"\t\n",
    "\n",
    "The TF-IDF approach is well suited for matching movie titles, since there are some words contained in titles that we would consider more important for matching compared to others. Articles such as \"The\" and \"A\" are worth keeping around for matching, as opposed to being removed as a stopword&mdash;a common early step in the NLP process, since titles may differ by only an article word (eg. [The Batman](https://www.imdb.com/title/tt1877830) vs. [Batman](https://www.imdb.com/title/tt0096895)). However, we still want to place less weight on articles and other common words. For example, [Playmobil: The Movie](https://www.imdb.com/title/tt4199898) should be more similar to \"Playmobil\" than to [Deadwood: The Movie](https://www.imdb.com/title/tt4943998). As mentioned above, because TF-IDF takes into account the distinctness of tokens among different documents (here each title is a document), we get this benefit of variable weights.\n",
    "\n",
    "For this specific exercise, my goal is to find the top candidate target (main title) for each of the MovieLens titles, and then compare that result with the true match. Building off of the work from Chris van den Berg's Blog Post, I created a python `StringMatch` class to handle the matching, with the following additional changes: \n",
    "- The class methods below are designed to take in two different lists (instead of matching within one list).\n",
    "- I added an option to return as either a dictionary or dataframe.\n",
    "- Instead of using a custom ngram function, I leverage sci-kit learn's built-in ngram tokenizer, which has padding for words less than three characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import re\n",
    "import time\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringMatch():\n",
    "    \n",
    "    def __init__(self, source_names, target_names):\n",
    "        self.source_names = source_names\n",
    "        self.target_names = target_names\n",
    "        self.ct_vect      = None\n",
    "        self.tfidf_vect   = None\n",
    "        self.vocab        = None\n",
    "        self.sprse_mtx    = None\n",
    "        \n",
    "        \n",
    "    def tokenize(self, analyzer='char_wb', n=3):\n",
    "        '''\n",
    "        Tokenizes the list of strings, based on the selected analyzer\n",
    "\n",
    "        :param str analyzer: Type of analyzer ('char_wb', 'word'). Default is trigram\n",
    "        :param str n: If using n-gram analyzer, the gram length\n",
    "        '''\n",
    "        # Create initial count vectorizer & fit it on both lists to get vocab\n",
    "        self.ct_vect = CountVectorizer(analyzer=analyzer, ngram_range=(n, n))\n",
    "        self.vocab   = self.ct_vect.fit(self.source_names + self.target_names).vocabulary_\n",
    "        \n",
    "        # Create tf-idf vectorizer\n",
    "        self.tfidf_vect  = TfidfVectorizer(vocabulary=self.vocab, analyzer=analyzer, ngram_range=(n, n))\n",
    "        \n",
    "        \n",
    "    def match(self, ntop=1, lower_bound=0, output_fmt='df'):\n",
    "        '''\n",
    "        Main match function. Default settings return only the top candidate for every source string.\n",
    "        \n",
    "        :param int ntop: The number of top-n candidates that should be returned\n",
    "        :param float lower_bound: The lower-bound threshold for keeping a candidate, between 0-1.\n",
    "                                   Default set to 0, so consider all canidates\n",
    "        :param str output_fmt: The output format. Either dataframe ('df') or dict ('dict')\n",
    "        '''\n",
    "        self._awesome_cossim_top(ntop, lower_bound)\n",
    "        \n",
    "        if output_fmt == 'df':\n",
    "            match_output = self._make_matchdf()\n",
    "        elif output_fmt == 'dict':\n",
    "            match_output = self._make_matchdict()\n",
    "            \n",
    "        return match_output\n",
    "        \n",
    "        \n",
    "    def _awesome_cossim_top(self, ntop, lower_bound):\n",
    "        ''' https://gist.github.com/ymwdalex/5c363ddc1af447a9ff0b58ba14828fd6#file-awesome_sparse_dot_top-py '''\n",
    "        # To CSR Matrix, if needed\n",
    "        A = self.tfidf_vect.fit_transform(self.source_names).tocsr()\n",
    "        B = self.tfidf_vect.fit_transform(self.target_names).transpose().tocsr()\n",
    "        M, _ = A.shape\n",
    "        _, N = B.shape\n",
    "\n",
    "        idx_dtype = np.int32\n",
    "\n",
    "        nnz_max = M * ntop\n",
    "\n",
    "        indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "        indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "        data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "        ct.sparse_dot_topn(\n",
    "            M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "            np.asarray(A.indices, dtype=idx_dtype),\n",
    "            A.data,\n",
    "            np.asarray(B.indptr, dtype=idx_dtype),\n",
    "            np.asarray(B.indices, dtype=idx_dtype),\n",
    "            B.data,\n",
    "            ntop,\n",
    "            lower_bound,\n",
    "            indptr, indices, data)\n",
    "\n",
    "        self.sprse_mtx = csr_matrix((data,indices,indptr), shape=(M,N))\n",
    "    \n",
    "    \n",
    "    def _make_matchdf(self):\n",
    "        ''' Build dataframe for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # COO matrix to list of tuples\n",
    "        match_list = []\n",
    "        for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "            match_list.append((row, self.source_names[row], col, self.target_names[col], val))\n",
    "\n",
    "        # List of tuples to dataframe\n",
    "        colnames = ['Row Idx', 'Title', 'Candidate Idx', 'Candidate Title', 'Score']\n",
    "        match_df = pd.DataFrame(match_list, columns=colnames)\n",
    "\n",
    "        return match_df\n",
    "\n",
    "    \n",
    "    def _make_matchdict(self):\n",
    "        ''' Build dictionary for result return '''\n",
    "        # CSR matrix -> COO matrix\n",
    "        cx = self.sprse_mtx.tocoo()\n",
    "\n",
    "        # dict value should be tuple of values\n",
    "        match_dict = {}\n",
    "        for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "            if match_dict.get(row):\n",
    "                match_dict[row].append((col,val))\n",
    "            else:\n",
    "                match_dict[row] = [(col, val)]\n",
    "\n",
    "        return match_dict   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the `StringMatch` class, we can run the matching algorithm using just a few lines of code (with default arguments):\n",
    "```\n",
    "titlematch = StringMatch(source_titles, target_titles)\n",
    "titlematch.tokenize()\n",
    "match_df = titlematch.match()\n",
    "```\n",
    "Let's take a look at how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IMDB Main Titles, filtering for movies\n",
    "imdb_data = pd.read_csv('data/title_basics.tsv', sep='\\t')\n",
    "imdb_data = (imdb_data\n",
    "             .dropna(subset=['primaryTitle'])\n",
    "             .query('titleType == \"movie\"')\n",
    "             .assign(title=imdb_data.primaryTitle + ' (' + imdb_data.startYear.astype(str) + ')'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>titleType</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>originalTitle</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>endYear</th>\n",
       "      <th>runtimeMinutes</th>\n",
       "      <th>genres</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0000009</td>\n",
       "      <td>movie</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>0</td>\n",
       "      <td>1894</td>\n",
       "      <td>\\N</td>\n",
       "      <td>45</td>\n",
       "      <td>Romance</td>\n",
       "      <td>Miss Jerry (1894)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>tt0000147</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Corbett-Fitzsimmons Fight</td>\n",
       "      <td>The Corbett-Fitzsimmons Fight</td>\n",
       "      <td>0</td>\n",
       "      <td>1897</td>\n",
       "      <td>\\N</td>\n",
       "      <td>20</td>\n",
       "      <td>Documentary,News,Sport</td>\n",
       "      <td>The Corbett-Fitzsimmons Fight (1897)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>tt0000335</td>\n",
       "      <td>movie</td>\n",
       "      <td>Soldiers of the Cross</td>\n",
       "      <td>Soldiers of the Cross</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>\\N</td>\n",
       "      <td>\\N</td>\n",
       "      <td>Biography,Drama</td>\n",
       "      <td>Soldiers of the Cross (1900)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>tt0000502</td>\n",
       "      <td>movie</td>\n",
       "      <td>Bohemios</td>\n",
       "      <td>Bohemios</td>\n",
       "      <td>0</td>\n",
       "      <td>1905</td>\n",
       "      <td>\\N</td>\n",
       "      <td>100</td>\n",
       "      <td>\\N</td>\n",
       "      <td>Bohemios (1905)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>tt0000574</td>\n",
       "      <td>movie</td>\n",
       "      <td>The Story of the Kelly Gang</td>\n",
       "      <td>The Story of the Kelly Gang</td>\n",
       "      <td>0</td>\n",
       "      <td>1906</td>\n",
       "      <td>\\N</td>\n",
       "      <td>70</td>\n",
       "      <td>Biography,Crime,Drama</td>\n",
       "      <td>The Story of the Kelly Gang (1906)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tconst titleType                   primaryTitle  \\\n",
       "8    tt0000009     movie                     Miss Jerry   \n",
       "145  tt0000147     movie  The Corbett-Fitzsimmons Fight   \n",
       "332  tt0000335     movie          Soldiers of the Cross   \n",
       "499  tt0000502     movie                       Bohemios   \n",
       "571  tt0000574     movie    The Story of the Kelly Gang   \n",
       "\n",
       "                     originalTitle  isAdult startYear endYear runtimeMinutes  \\\n",
       "8                       Miss Jerry        0      1894      \\N             45   \n",
       "145  The Corbett-Fitzsimmons Fight        0      1897      \\N             20   \n",
       "332          Soldiers of the Cross        0      1900      \\N             \\N   \n",
       "499                       Bohemios        0      1905      \\N            100   \n",
       "571    The Story of the Kelly Gang        0      1906      \\N             70   \n",
       "\n",
       "                     genres                                 title  \n",
       "8                   Romance                     Miss Jerry (1894)  \n",
       "145  Documentary,News,Sport  The Corbett-Fitzsimmons Fight (1897)  \n",
       "332         Biography,Drama          Soldiers of the Cross (1900)  \n",
       "499                      \\N                       Bohemios (1905)  \n",
       "571   Biography,Crime,Drama    The Story of the Kelly Gang (1906)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>imdbId</th>\n",
       "      <th>tmdbId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>tt0114709</td>\n",
       "      <td>862.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children|Fantasy</td>\n",
       "      <td>tt0113497</td>\n",
       "      <td>8844.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "      <td>tt0113228</td>\n",
       "      <td>15602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>31357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>tt0113041</td>\n",
       "      <td>11862.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                               title  \\\n",
       "0        1                    Toy Story (1995)   \n",
       "1        2                      Jumanji (1995)   \n",
       "2        3             Grumpier Old Men (1995)   \n",
       "3        4            Waiting to Exhale (1995)   \n",
       "4        5  Father of the Bride Part II (1995)   \n",
       "\n",
       "                                        genres     imdbId   tmdbId  \n",
       "0  Adventure|Animation|Children|Comedy|Fantasy  tt0114709    862.0  \n",
       "1                   Adventure|Children|Fantasy  tt0113497   8844.0  \n",
       "2                               Comedy|Romance  tt0113228  15602.0  \n",
       "3                         Comedy|Drama|Romance  tt0114885  31357.0  \n",
       "4                                       Comedy  tt0113041  11862.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Source of MovieLens Movie Title List\n",
    "# https://www.kaggle.com/grouplens/movielens-latest-full\n",
    "\n",
    "# Load the MovieLens Dataset & filter for latin characters\n",
    "mvlens_ttl = pd.read_csv('data/movies.csv')\n",
    "mvlens_links = pd.read_csv('data/links.csv')\n",
    "\n",
    "# Join MovieLens titles to links\n",
    "mvlens_ttl_j = (mvlens_ttl\n",
    "                # Remove titles without latin characters\n",
    "                .assign(title=mvlens_ttl.title.apply(lambda x: x.encode('ascii', 'ignore').decode('utf-8').strip()))\n",
    "                .query('title != \"\"')\n",
    "                # Join to links for IMDB Id SOT\n",
    "                .merge(mvlens_links, how='inner', on='movieId')\n",
    "               )\n",
    "\n",
    "\n",
    "# Tweak imdbid formatting for join later\n",
    "mvlens_ttl_j.imdbId = mvlens_ttl_j.imdbId.apply(lambda x: 'tt' + str(x).zfill(7))\n",
    "\n",
    "# Get list of Titles and IDs\n",
    "mvlens_ttl_list = mvlens_ttl_j.title.tolist()\n",
    "mvlens_id_list  = mvlens_ttl_j.movieId.tolist()\n",
    "\n",
    "# Let's take a look at the combined MovieLens table\n",
    "mvlens_ttl_j.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Get list of Primary Titles and IDs\n",
    "imdb_ttl_list = imdb_data.title.tolist()\n",
    "imdb_id_list  = imdb_data.tconst.tolist()\n",
    "imdb_ttl_dict = dict(zip(imdb_id_list, imdb_ttl_list))\n",
    "\n",
    "# Match the MvLens titles to IMDB titles\n",
    "titlematch = StringMatch(mvlens_ttl_list, imdb_ttl_list)\n",
    "titlematch.tokenize()\n",
    "match_df = titlematch.match()\n",
    "\n",
    "# Small formatting changes & merge w/ SOT\n",
    "match_df = (match_df\n",
    "            # Attach the IMDB Id for the MVLens\n",
    "            .assign(mvlens_id=mvlens_id_list)\n",
    "            # Lookup & attach the IMDB for the top candidate title\n",
    "            .assign(candidate_imdb_id=match_df['Candidate Idx'].apply(lambda x: imdb_id_list[x]))\n",
    "            #  Join to SOT - Main Titles\n",
    "            .merge(mvlens_ttl_j, left_on='mvlens_id', right_on='movieId')\n",
    "            # Drop unnecessary columns\n",
    "            .drop(columns=['movieId', 'title', 'genres', 'tmdbId']))\n",
    "\n",
    "# Modify the merged df\n",
    "match_df = (match_df\n",
    "            # Add actual IMDB Title, using the key provided by MovieLens\n",
    "            .assign(imdbTitle=match_df.imdbId.apply(lambda x: imdb_ttl_dict.get(x)))\n",
    "            # Compare candidate IMDB Id w/ actual title IMDB Id for accuracy calculation\n",
    "            .assign(acc=np.where(match_df.candidate_imdb_id == match_df.imdbId, 1, 0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate was our match process? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The true positive match rate is 0.7579784487210384\n"
     ]
    }
   ],
   "source": [
    "print(f\"The true positive match rate is {match_df.acc.sum() / len(match_df.index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it compare to the Levenshtein Distance algorithm? Let's compare by applying the fuzzywuzzy package to the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 3]\n",
      "['three', 'six', 'four']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# items = range(0, 10)\n",
    "# rand_items = random.sample(items, 3)\n",
    "# print(rand_items)\n",
    "\n",
    "test = ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight']\n",
    "sample_idx = random.sample(range(0, len(test)), 3)\n",
    "print(sample_idx)\n",
    "sampled_list = [test[i] for i in sample_idx]\n",
    "print(sampled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from random import sample \n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Code below from https://galaxydatatech.com/2017/12/31/fuzzy-string-matching-pandas-fuzzywuzzy/\n",
    "def match_names_fuzzywuzzy(source_list, target_list):\n",
    "    \n",
    "    name_match=[]\n",
    "    ratio_match=[]\n",
    "\n",
    "    for row in source_list:\n",
    "        x = process.extractOne(row, target_list)\n",
    "        name_match.append(x[0])\n",
    "        ratio_match.append(x[1])\n",
    "        \n",
    "    df = pd.DataFrame(list(zip(source_list, name_match, ratio_match)), \n",
    "               columns =['Source Title', 'Candidate Title', 'Score']) \n",
    "    return df\n",
    "\n",
    "\n",
    "def time_match_sample(source, target, sample_size):\n",
    "    '''\n",
    "    Time the match process on a sample of data, using both the tf-idf and fuzzywuzzy process.\n",
    "\n",
    "    :param list source: A list of strings to match\n",
    "    :param list target: A list of strings to match to\n",
    "    :param int sample_size: The number of items that should be sampled from the source\n",
    "    \n",
    "    Returns: A tuple with the time (sec) to process the match (tf-idf, fuzzywuzzy)\n",
    "    '''\n",
    "    \n",
    "    # Sample the source input\n",
    "    sample_idx = random.sample(range(0, len(source)), sample_size)\n",
    "    source_sample = [source[i] for i in sample_idx]\n",
    "    \n",
    "    # Run TF-IDF\n",
    "    t0 = datetime.now()\n",
    "    titlematch = StringMatch(source_sample, target)\n",
    "    titlematch.tokenize()\n",
    "    match_df = titlematch.match()\n",
    "    \n",
    "    # Fuzzywuzzy implmentation\n",
    "    t1 = datetime.now()\n",
    "    df_fuzzywuzzy = match_names_fuzzywuzzy(source_sample, target)\n",
    "    t2 = datetime.now()\n",
    "    \n",
    "    print(\"Complete.\")\n",
    "    return ((t1-t0).total_seconds(), (t2-t1).total_seconds())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n",
      "Complete.\n",
      "Complete.\n",
      "Complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\process.py\u001b[0m in \u001b[0;36mextractWithoutOrder\u001b[1;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# See if choices is a dictionary-like object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchoices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-42ef658c6923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msample_10\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mtime_match_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvlens_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimdb_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msample_100\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtime_match_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvlens_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimdb_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msample_1000\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime_match_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvlens_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimdb_ttl_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-dbe77e3027e0>\u001b[0m in \u001b[0;36mtime_match_sample\u001b[1;34m(source, target, sample_size)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;31m# Fuzzywuzzy implmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mdf_fuzzywuzzy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch_names_fuzzywuzzy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-dbe77e3027e0>\u001b[0m in \u001b[0;36mmatch_names_fuzzywuzzy\u001b[1;34m(source_list, target_list)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractOne\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mname_match\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mratio_match\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\process.py\u001b[0m in \u001b[0;36mextractOne\u001b[1;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[0;32m    218\u001b[0m     \u001b[0mbest_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextractWithoutOrder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchoices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_cutoff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\process.py\u001b[0m in \u001b[0;36mextractWithoutOrder\u001b[1;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mchoice\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchoices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mprocessed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpre_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mscore_cutoff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py\u001b[0m in \u001b[0;36mWRatio\u001b[1;34m(s1, s2, force_ascii, full_process)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtry_partial\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mpartial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpartial_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mptsor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial_token_sort_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_process\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;33m*\u001b[0m \u001b[0munbase_scale\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mpartial_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\utils.py\u001b[0m in \u001b[0;36mdecorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py\u001b[0m in \u001b[0;36mpartial_ratio\u001b[1;34m(s1, s2)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Time the runs for each method at different sample sizes\n",
    "sample_1    = time_match_sample(mvlens_ttl_list, imdb_ttl_list, 1)\n",
    "sample_5    = time_match_sample(mvlens_ttl_list, imdb_ttl_list, 5)\n",
    "sample_10   = time_match_sample(mvlens_ttl_list, imdb_ttl_list, 10)\n",
    "sample_100  = time_match_sample(mvlens_ttl_list, imdb_ttl_list, 100)\n",
    "sample_1000 = time_match_sample(mvlens_ttl_list, imdb_ttl_list, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn chart below\n",
    "# x-axis is size of dataset\n",
    "# y-axis is the time to process\n",
    "# https://seaborn.pydata.org/generated/seaborn.lineplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here i need to show a chart with the difference in accuracy and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO:\n",
    " ~~-- 1. Finish the new class in the TF-IDF notebook~~-\n",
    " - 2. Compare it to running fuzzy on everything, using this code: https://galaxydatatech.com/2017/12/31/fuzzy-string-matching-pandas-fuzzywuzzy/\n",
    " - 3. chart showing increase in dataset size and then corresponding runtime, by method\n",
    " \n",
    " \n",
    " - 1. Time the code, using our vectorized approach vs. fuzzy-wuzzy, and using different token patterns\n",
    " - 2. Calculate the match rate (after the join), using different approaches\n",
    " - 3. Follow-Up: Do the same version, but using Spark instead\n",
    " ~~- 4. Make sure I follow my steps from TF-IDF (Magic)~~-\n",
    " - 5. Maybe test different versions of the ngram length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB Data Source: https://www.imdb.com/interfaces/  \n",
    "Information courtesy of IMDb (http://www.imdb.com).  \n",
    "Used with permission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
