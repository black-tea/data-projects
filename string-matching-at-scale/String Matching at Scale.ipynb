{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy String Matching at Scale\n",
    "\n",
    "### Entity Resolution is Everywhere, and Sometimes All We Got Are Strings\n",
    "![StringUrl](https://media.giphy.com/media/l3JDHiU3rdY4oBK3S/giphy.gif \"string\")  \n",
    "More and more often, companies are combining data from different sources to enhance and enrich the value we are getting from the data. Central to this effort is the concept of entity resolution (or record linkage) to ensure that we are looking at the same record across multiple different sources. In some cases the records may have enough different types of information that can be used to build a probabilistic estimate on whether it is the same entity. In other cases, we may only be looking at one field, such as a name, and we need to decide whether it is enough of a match or not.\n",
    "\n",
    "#### More Data = Need for Speed\n",
    "Fuzzy string matching is not a new problem, and several algorithms are commonly employed (levenstein distance, However, given the growth in the number of data that are being matched, it is increasingly important to be able to perform this matching at scale. Instead of comparing every record to every possible match, we can employ a vectorized approach. Natural Language Processing (NLP) libraries make this not only possible, but relatively painless in implementation.\n",
    "\n",
    "#### TF-IDF Approach\n",
    "I got excited about using the NLP toolkit for short string matching after reading about its success on [Chris van den Berg's Blog Post](https://bergvca.github.io/2017/10/14/super-fast-string-matching.html). TF-IDF stands for \"term frequency-inverse document frequency\" and is a common approach to measuring similarity/dissimilarity among documents in a corpus (a collection of documents). The TF-IDF calculations typically consist of the following steps:\n",
    "1. Pre-processing & Tokenization: Perform any cleaning on the data (case conversion, removal of stopwords & punctuation) and convert each document into tokens. Although this is typically done at the word level, we have the flexibility to define a token at a lower level, such as an n-gram, which is more useful for short string matching since we might only have a few words in each string.\n",
    "2. Calculate the Term Frequency: The purpose of this step is to determine which words define the document. For each document (a string in our case), calculate the frequency for each term (token) in the document and divide by the total number of terms in the document. If we define a token as an n-gram, we will calculate the frequency of  \n",
    "3. Calculate the Inverse Document Frequency: The purpose of this step is to calculate the appropriate *weight* for each term, depending on how often it appears across all documents. A term that appears in all the different documents will have a lower weight compared to a term that only appears in one of the documents. The idea is that a token that appears in all documents is less is less descriptive of any particular document compared to a token that appears in only one of the documents.\n",
    "4. Calculate the Cosine Similarity: As described by Chris van den Berg, data scientists at ING developed a custom library to make the cosine similarity calcualtions faster than the built-in sci-kit learn implementation. We will use this library for matching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Matching Film Titles\n",
    "Matching titles is a perfect use case, since in many cases there may not be much more than a name to use for matching and we need to find the best match against a medium-large data set. For this example, I will demonstrate the TF-IDF string matching approach using the IMDB title dataset, joining title aliases to the main titles. The IMDB set is ideal for two reasons: \n",
    "1. Often title aliases are very similar to the original title name\n",
    "2. We can join the tables to get the true positive match for every record\n",
    "\n",
    "The TF-IDF approach is also well suited to movie titles, since there are some words contained in titles that we would consider more important for matching compared to others. Articles such as \"The\" and \"A\" are worth keeping around for matching, as opposed to being removed as a stopword&mdash;a common early step in the NLP process, since titles may differ by only an article word (eg. [The Batman](https://www.imdb.com/title/tt1877830) vs. [Batman](https://www.imdb.com/title/tt0096895)) However, we still want to place less weight on articles and other common words. For example, [Playmobil: The Movie](https://www.imdb.com/title/tt4199898) should be more similar to \"Playmobil\" than to [Deadwood: The Movie](https://www.imdb.com/title/tt4943998). As mentioned above, because TF-IDF takes into account the distinctness of tokens among different documents (here each title is a document), we get this benefit of variable weights.\n",
    "\n",
    "For this specific exercise, my goal is to find the top candidate target (main title) for each of the alias titles, and then compare that result with the true positive match. Building off of the work from Chris van den Berg's Blog Post, I created a python `TitleMatch` class to handle the matching, with the following additional changes: \n",
    "- The class methods below are designed to take in two different lists (instead of matching within one list).\n",
    "- I added an option to return as either a dictionary or dataframe.\n",
    "- Instead of using a custom ngram function, I leverage sci-kit learn's built-in ngram tokenizer, which has padding for words less than three characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import sparse_dot_topn.sparse_dot_topn as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Functions\n",
    "\n",
    "def ngrams(string, n=3):\n",
    "    ngrams = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def awesome_cossim_top(A, B, ntop, lower_bound=0):\n",
    "    # force A and B as a CSR matrix.\n",
    "    # If they have already been CSR, there is no overhead\n",
    "    A = A.tocsr()\n",
    "    B = B.tocsr()\n",
    "    M, _ = A.shape\n",
    "    _, N = B.shape\n",
    " \n",
    "    idx_dtype = np.int32\n",
    " \n",
    "    nnz_max = M*ntop\n",
    " \n",
    "    indptr = np.zeros(M+1, dtype=idx_dtype)\n",
    "    indices = np.zeros(nnz_max, dtype=idx_dtype)\n",
    "    data = np.zeros(nnz_max, dtype=A.dtype)\n",
    "\n",
    "    ct.sparse_dot_topn(\n",
    "        M, N, np.asarray(A.indptr, dtype=idx_dtype),\n",
    "        np.asarray(A.indices, dtype=idx_dtype),\n",
    "        A.data,\n",
    "        np.asarray(B.indptr, dtype=idx_dtype),\n",
    "        np.asarray(B.indices, dtype=idx_dtype),\n",
    "        B.data,\n",
    "        ntop,\n",
    "        lower_bound,\n",
    "        indptr, indices, data)\n",
    "\n",
    "    return csr_matrix((data,indices,indptr),shape=(M,N))\n",
    "\n",
    "\n",
    "def make_matchdf(sparse_matrix, A_vec, B_vec):\n",
    "    # CSR matrix -> COO matrix\n",
    "    cx = sparse_matrix.tocoo()\n",
    "    \n",
    "    # COO matrix to list of tuples\n",
    "    match_list = []\n",
    "    for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "        match_list.append((row, A_vec[row], col, B_vec[col], val))\n",
    "    \n",
    "    # List of tuples to dataframe\n",
    "    colnames = ['Row Idx', 'Title', 'Candidate Idx', 'Candidate Title', 'Score']\n",
    "    match_df = pd.DataFrame(match_list, columns=colnames)\n",
    "    \n",
    "    return match_df\n",
    "\n",
    "\n",
    "def make_matchdict(sparse_matrix, A_vec, B_vec):\n",
    "    # CSR matrix -> COO matrix\n",
    "    cx = sparse_matrix.tocoo()\n",
    "    \n",
    "    # dict value should be tuple of values\n",
    "    match_dict = {}\n",
    "    for row,col,val in zip(cx.row, cx.col, cx.data):\n",
    "        if match_dict.get(row):\n",
    "            match_dict[row].append((col,val))\n",
    "        else:\n",
    "            match_dict[row] = [(col, val)]\n",
    "    \n",
    "    return match_dict\n",
    "\n",
    "\n",
    "def get_top_matches(matchdict):\n",
    "    # Find the max value in the match dict\n",
    "    max_dict = {}\n",
    "    for target_key, scores in match_dict.items():\n",
    "\n",
    "        # For each movie, get top candidate\n",
    "        max_score = max(scores, key=operator.itemgetter(1))[1]\n",
    "\n",
    "        # In case of ties, keep all\n",
    "        matches = [score for score in scores if (score[1] == max_score)]\n",
    "        max_dict[target_key] = matches\n",
    "\n",
    "    return max_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate Function\n",
    "def match_names(source_names, target_names, ntop=10, lower_bound=0.7, analyzer='word', returntype='dict'):\n",
    "    \n",
    "    # Set the timer\n",
    "    t1 = time.time()\n",
    "\n",
    "    # Create initial count vectorizer & fit it on both lists\n",
    "    if analyzer == '3gram':\n",
    "        ct_vect = CountVectorizer(analyzer='char_wb', ngram_range=(3, 3))\n",
    "    elif analyzer == 'word':\n",
    "        ct_vect = CountVectorizer(analyzer='word')\n",
    "    elif analyzer == 'ngram':\n",
    "        ct_vect = CountVectorizer(analyzer=ngrams)\n",
    "    vocab = ct_vect.fit(source_names + target_names).vocabulary_\n",
    "    \n",
    "    # Create tf-idf vectorizer\"\n",
    "    if analyzer == '3gram':\n",
    "        tfidf_vect = TfidfVectorizer(vocabulary=vocab, analyzer='char_wb', ngram_range=(3, 3))\n",
    "    elif analyzer == 'word':\n",
    "        tfidf_vect = TfidfVectorizer(vocabulary=vocab, analyzer='word')\n",
    "    elif analyzer == 'ngram':\n",
    "        tfidf_vect = TfidfVectorizer(vocabulary=vocab, analyzer=ngrams)\n",
    "    \n",
    "    source_names_tfidf_mat = tfidf_vect.fit_transform(source_names)\n",
    "    target_names_tfidf_mat = tfidf_vect.transform(target_names)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    # Get matches, convert to df\n",
    "    matches = awesome_cossim_top(source_names_tfidf_mat,\n",
    "                                 target_names_tfidf_mat.transpose(),\n",
    "                                 ntop,\n",
    "                                 lower_bound)\n",
    "    if returntype == 'df':\n",
    "        match_output = make_matchdf(matches, source_names, target_names)\n",
    "    elif returntype == 'dict':\n",
    "        match_output = make_matchdict(matches, source_names, target_names)\n",
    "    t3 = time.time()\n",
    "    \n",
    "    # Print out run summary\n",
    "    print(f\"time to create vectorizer: {t2-t1}\")\n",
    "    print(f\"time to calculate cosine similarity: {t3-t2}\")\n",
    "    print(f\"runtime: {time.time() - t1}\")\n",
    "    \n",
    "    return match_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import IMDB Main Titles, filtering for movies\n",
    "imdb_data = pd.read_csv('data/title_basics.tsv', sep='\\t')\n",
    "imdb_movies = imdb_data[imdb_data['titleType'] == 'movie']\n",
    "imdb_movie_titles = imdb_movies['primaryTitle'].reset_index(drop=True).tolist()\n",
    "print(f\"Target Count: {len(imdb_movie_titles)}\")\n",
    "      \n",
    "# Import AKAs\n",
    "imdb_akas = pd.read_csv('data/title_akas.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " TODO:\n",
    " - 1. Finish the new class in the TF-IDF notebook\n",
    " - 2. Compare it to running fuzzy on everything, using this code: https://galaxydatatech.com/2017/12/31/fuzzy-string-matching-pandas-fuzzywuzzy/\n",
    " \n",
    " \n",
    " - 1. Time the code, using our vectorized approach vs. fuzzy-wuzzy, and using different token patterns\n",
    " - 2. Calculate the match rate (after the join), using different approaches\n",
    " - 3. Follow-Up: Do the same version, but using Spark instead\n",
    " - 4. Make sure I follow my steps from TF-IDF (Magic)\n",
    " - 5. Maybe test different versions of the ngram length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMDB Data Source: https://www.imdb.com/interfaces/  \n",
    "Information courtesy of IMDb (http://www.imdb.com).  \n",
    "Used with permission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
